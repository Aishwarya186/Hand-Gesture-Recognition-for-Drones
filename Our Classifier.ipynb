{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "#from tensorflow.keras import backend\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import random, time, os, json, glob, re, time\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import datetime\n",
    "\n",
    "% matplotlib inline\n",
    "from usefull_function import get_all_labels, get_class, get_id_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, json, glob\n",
    "\n",
    "\n",
    "\n",
    "def get_id_label_dict(train_labels,val_labels):\n",
    "    labels = pd.read_csv(train_labels, index_col=0, sep=';', header=None)\n",
    "    labels = json.loads(labels.to_json())['1']\n",
    "    validation_labels = pd.read_csv(val_labels, index_col=0, sep=';', header=None)\n",
    "    validation_labels = json.loads(validation_labels.to_json())['1']\n",
    "    labels.update(validation_labels)\n",
    "    #print(labels)\n",
    "    return labels\n",
    "\n",
    "def get_all_labels(all_labels):\n",
    "    df = pd.read_csv(all_labels, header = None).reset_index()\n",
    "    label_class_dict = dict(df[[0,'index']].values)\n",
    "    #print(label_class_dict)\n",
    "    return label_class_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_class(file, id_label_dict, label_class_dict):\n",
    "    f_id = re.findall(r'([0-9]*big).(npy|json)', file)[0][0]\n",
    "    f_id = f_id[:-3]\n",
    "    #print(f_id)\n",
    "    f_label = id_label_dict[f_id]\n",
    "    f_class = label_class_dict[f_label]\n",
    "    #print(\"cl\", f_class)\n",
    "    return f_class\n",
    "\n",
    "\n",
    "def make_numpy_array(file, db_type):\n",
    "    if db_type == 'all':\n",
    "        result = np.zeros((37,120))\n",
    "    elif db_type == 'body':\n",
    "        result = np.zeros((37,36))\n",
    "    elif db_type == 'hands':\n",
    "        result = np.zeros((37,84))\n",
    "  \n",
    "    for k in sorted(list(file['data'].keys())):\n",
    "        k_int = int(k)\n",
    "        try:\n",
    "            data = file['data'][k][0]\n",
    "\n",
    "            if db_type == 'all':\n",
    "                bla = np.r_[data['pose_keypoints'], data['hand_left_keypoints'], data['hand_right_keypoints']]\n",
    "            if db_type == 'body':\n",
    "                bla = data['pose_keypoints']\n",
    "            if db_type == 'hands':\n",
    "                bla = np.r_[data['hand_left_keypoints'], data['hand_right_keypoints']]\n",
    "\n",
    "            bla = bla[[False if i %3 ==2 else True for i in range(len(bla))]]\n",
    "            result[k_int-1] = bla\n",
    "        except:\n",
    "            None\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some general information we need to handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many frames there are in a video sequence, we feed to the RNN\n",
    "n_steps = 37\n",
    "\n",
    "# How many different classes there are.\n",
    "n_classes = 4\n",
    "\n",
    "# Which dataset are we looking at, choices body | hands | all\n",
    "db_type = 'body'\n",
    "\n",
    "# Where are the csv stored.\n",
    "train_labels = '../labels/train.csv'\n",
    "val_labels = '../labels/validation.csv'\n",
    "all_labels = '../labels/labels.csv'\n",
    "\n",
    "\n",
    "labels = get_id_label_dict(train_labels, val_labels)\n",
    "#print(labels)\n",
    "label_class_dict = get_all_labels(all_labels)\n",
    "#print(label_class_dict)\n",
    "file_path = join('../train/',db_type,'*.npy')\n",
    "#print(file_path)\n",
    "files = glob.glob(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data, shuffling and splitting\n",
    "\n",
    "Note we can save the data in a numpy array since our dataset is not too big. \n",
    "Otherwise we need to work with an iterator that gets the array iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 3 0 2 2 0 1 3 0 2 2 1 1 2 1 1 2 1 2 2 0 0 3 0 1 0 3 3 0 2 1 3 1 2 0 0\n",
      " 3 1 0 2 0 3 1 0 3 3 1 2 1 1 2 0 1 2 1 0 2 1 3 2 2 0 1 2 3 0 3 3 3 0 3 0 1\n",
      " 0 3 0 0 1 3 2 2 0 1 3 2 3 1 0 3 1 1 1 1 3 2 1 0 1 3 2 3 0 3 2 2 3 3 0 2 2\n",
      " 0 2 1 0 2 3 3 0 3 3]\n"
     ]
    }
   ],
   "source": [
    "y_data = np.array([get_class(file, labels, label_class_dict) for file in files])\n",
    "print(y_data)\n",
    "y_original = y_data\n",
    "# Make a one hot encoding of the labels\n",
    "y_tmp = np.zeros((len(y_data), n_classes), dtype = np.int32)\n",
    "for i in range(len(y_data)):\n",
    "    y_tmp[i,y_data[i]] = 1\n",
    "\n",
    "y_data = y_tmp\n",
    "  \n",
    "X_data = np.r_[[np.load(files[i]) for i in range(len(files))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44,  47,   4,  55,  26,  64,  73,  10,  40, 108])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 120\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "permute = np.random.permutation(y_data.shape[0])\n",
    "y_data = y_data[permute]\n",
    "X_data = X_data[permute]\n",
    "\n",
    "y_train, y_test = y_data[:train_size], y_data[train_size:]\n",
    "X_train, X_test = X_data[:train_size], X_data[train_size:]\n",
    "y_test_true = y_original[permute][train_size:] \n",
    "permute[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 44,  47,   4,  55,  26,  64,  73,  10,  40, 108])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph \n",
    "\n",
    "## Some general specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data \n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Some graph specific numbers\n",
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 1197 test series\n",
    "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
    "n_hidden = [90,50] # Hidden layer num of features\n",
    "\n",
    "\n",
    "\n",
    "# calculated as: decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "decaying_learning_rate = True\n",
    "learning_rate = 0.0025 #used if decaying_learning_rate set to False\n",
    "init_learning_rate = 0.002\n",
    "decay_rate = 0.96 #the base of the exponential in the decay\n",
    "decay_steps = 100000 #used in decay every 60000 steps with a base of 0.96\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "epochs = 50\n",
    "training_iters = training_data_count *epochs  # Loop number of epoch through the data set.\n",
    "batch_size = 512\n",
    "display_iter = batch_size*64 # To show test set accuracy during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
    "    # Return a shuffled batch\n",
    "    \n",
    "    index = np.random.choice(_unsampled,size = batch_size)\n",
    "    batch_s = _train[index]\n",
    "    batch_labels = _labels[index]\n",
    "    _unsampled = np.delete(_unsampled, index)\n",
    "    \n",
    "    return batch_s, batch_labels, _unsampled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## The graph definition\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.int32, [None, n_classes])\n",
    "#y_hot = tf.one_hot(tf.cast(y,tf.int32),n_classes)\n",
    "\n",
    "\n",
    "with tf.name_scope('Input') as _:\n",
    "    xt = tf.transpose(x, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    xr = tf.reshape(xt, [-1, n_input])  \n",
    "  # Rectifies Linear Unit activation function used\n",
    "    \n",
    "with tf.name_scope('First_fc_layer') as _:\n",
    "    y_1 = tf.layers.dense(xr, n_hidden[0], \n",
    "    bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                  activation=tf.nn.relu, name='layer_1')\n",
    "    \n",
    "  # So the reader might be wondering why we take the transpose before the reshape\n",
    "  # This has to do with how the lstm wants to eat the data.\n",
    "  # the lstm want n_frames tensors, where each tensor is of size batchsize x num_features\n",
    "  # So basically first all the 1st frames then all the 2nd frames etc.\n",
    "    _X_split = tf.split(y_1, n_steps, 0) \n",
    "\n",
    "    \n",
    "with tf.name_scope('LSTM') as _:\n",
    "  # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden[0], forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X_split, dtype=tf.float32)\n",
    "    \n",
    "  # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    \n",
    "with tf.name_scope('Second_fc_layer') as _:\n",
    "    y_2 = tf.layers.dense(lstm_last_output, n_hidden[1], \n",
    "                          bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                          activation=tf.nn.relu, name='layer_2')\n",
    "    \n",
    "    \n",
    "with tf.name_scope('Last_fc_layer') as _:\n",
    "    out = tf.layers.dense(y_2, n_classes, \n",
    "                                  bias_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.1), \n",
    "                                  activation=None, \n",
    "                                  name='final_layer')\n",
    "\n",
    "\n",
    "with tf.name_scope('Output') as _:\n",
    "    pred = out\n",
    "\n",
    "with tf.name_scope('Optimizer') as _:\n",
    "  # Loss, optimizer and evaluation\n",
    "  # The regularization term\n",
    "    l2 = lambda_loss_amount * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()) \n",
    "  \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                          labels=y, \n",
    "                          logits=pred)) + l2 # Softmax loss\n",
    "    if decaying_learning_rate:\n",
    "        learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "  #decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
    "    tf.summary.scalar('cost', cost)\n",
    "  \n",
    "  \n",
    "with tf.name_scope('Accuracy') as _:\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Training of the Graph\n",
    "\n",
    "## Only run this part if you want to train your own model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititialize writers\n",
    "time_now = datetime.datetime.strftime(datetime.datetime.now(), '%H%M%S')\n",
    "summary_dir = './logs/'+db_type+time_now\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(summary_dir + '/train')\n",
    "test_writer=  tf.summary.FileWriter(summary_dir + '/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter: 512,  Accuracy test: 1.0, Accuracy train: 0.255859375, time: 2.966365337371826\n",
      "\tbetter network stored, 1.0 > 0\n",
      "Optimization Finished!\n",
      "INFO:tensorflow:Restoring parameters from ./logs/body234307/bestNetwork\n",
      "best training accuracy: 0.24166666 best test accuracy:  1.0\n",
      "TOTAL TIME:  10.493135690689087\n"
     ]
    }
   ],
   "source": [
    "#sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Perform Training steps with \"batch_size\" amount of data at each loop. \n",
    "# Elements of each batch are chosen randomly, without replacement, from X_train, \n",
    "# restarting when remaining datapoints < batch_size\n",
    "step = 1\n",
    "best_test_acc = 0\n",
    "time_start = time.time()\n",
    "unsampled_indices = list(range(0,len(X_train)))\n",
    "start = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.graph.finalize()\n",
    "    writer = tf.summary.FileWriter(summary_dir, sess.graph)\n",
    "    while step * batch_size <= training_iters:\n",
    "        if len(unsampled_indices) < batch_size:\n",
    "            unsampled_indices = list(range(0,len(X_train))) \n",
    "        batch_xs, batch_ys, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
    "    \n",
    "    # Fit training using batch data\n",
    "        _, loss_trn, acc_trn = sess.run([optimizer, cost, accuracy], feed_dict={x: batch_xs, y: batch_ys})\n",
    "    \n",
    "        summary = sess.run(merged, feed_dict={x: batch_xs, y: batch_ys}) \n",
    "        train_writer.add_summary(summary, step)\n",
    "    \n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "        if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "            loss_tst, acc_tst = sess.run([cost, accuracy], feed_dict={x: X_test, y: y_test})\n",
    "            summary = sess.run(merged, feed_dict={x: X_test, y: y_test})\n",
    "        \n",
    "            test_writer.add_summary(summary, step)\n",
    "        \n",
    "        \n",
    "            print('\\r Iter: {},  Accuracy test: {}, Accuracy train: {}, time: {}'.format(step*batch_size, acc_tst, acc_trn, time.time()-start))\n",
    "        \n",
    "            if(acc_tst > best_test_acc):\n",
    "                print(\"\\tbetter network stored,\", acc_tst, \">\", best_test_acc)\n",
    "                best_test_acc = acc_tst\n",
    "                saver.save(sess=sess, save_path=summary_dir + '/bestNetwork')\n",
    "          \n",
    "        \n",
    "            start = time.time()\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "  # Accuracy for test data\n",
    "  \n",
    "    saver.restore(sess=sess, save_path=summary_dir + '/bestNetwork')\n",
    "    print(\"best training accuracy:\", sess.run(accuracy, feed_dict={x: X_train, y: y_train}),\n",
    "          \"best test accuracy: \", sess.run(accuracy, feed_dict={x: X_test, y: y_test}))\n",
    "\n",
    "time_stop = time.time()\n",
    "print((\"TOTAL TIME:  {}\".format(time_stop - time_start)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
